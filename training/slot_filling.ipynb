{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "import json\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import evaluate\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, get_scheduler"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bebe48c961c8db5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "seed_everything(22)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f4abf86717b830c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21b282d727512b78"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SlotFillingConfig:\n",
    "    name_to_tag = {\n",
    "        'category': 'CAT',\n",
    "        'brand': 'BRAND',\n",
    "        'model': 'MODEL',\n",
    "        'price': 'PRICE',\n",
    "        'rating': 'RAT',\n",
    "    }\n",
    "    tag_to_name = {v: k for k, v in name_to_tag.items()}\n",
    "    tags = list(tag_to_name.keys())\n",
    "    null_label = 'O'\n",
    "    labels = sorted([null_label] + [f'B-{tag}' for tag in tags] + [f'I-{tag}' for tag in tags], \n",
    "                    key=lambda s: s[2] + s[0] if len(s) > 2 else '1')\n",
    "    id_to_label = dict(enumerate(labels))\n",
    "    label_to_id = {v: k for k, v in id_to_label.items()}\n",
    "    num_labels = len(labels)\n",
    "\n",
    "config = SlotFillingConfig()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "861f759353d41fb4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkpoint = 'google-bert/bert-base-multilingual-cased'\n",
    "\n",
    "tokenizer_kwargs = dict(return_tensors='pt', max_length=64, truncation=True, padding='max_length')\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7738a37e88799de"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pattern = r'|'.join(fr'({tag})' for tag in config.tags)\n",
    "\n",
    "queries_product_search = Path('raw_data/product_search.txt').read_text().split('\\n')\n",
    "queries_product_info = Path('raw_data/product_info.txt').read_text().split('\\n')\n",
    "\n",
    "slot_filling_dataset = []           # text, tokens, labels\n",
    "retrieval_dataset = []              # text, product\n",
    "intent_classification_dataset = []  # text, intent\n",
    "\n",
    "def label_data(data):\n",
    "    \"\"\"\n",
    "    Create labeled tokenized dataset from raw data\n",
    "    \n",
    "    Args:\n",
    "        data: input\n",
    "    Returns:\n",
    "        tuple (text, tokens, labels)\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\((.+?)\\)[A-Z]+', r'\\1', data)\n",
    "    token_ids = tokenizer(text, **tokenizer_kwargs)['input_ids'][0]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    labels = [config.null_label for _ in range(len(tokens))]\n",
    "    slots = {}\n",
    "    matched_tag = config.null_label\n",
    "    i_data = 0\n",
    "    i_tokens = 0\n",
    "    while i_data < len(data) and i_tokens < len(tokens):\n",
    "        data_char = data[i_data]\n",
    "        token = tokens[i_tokens]\n",
    "        if token in ('[CLS]', '[SEP]'):\n",
    "            print(f'Technical token {token!r}')\n",
    "            i_tokens += 1\n",
    "        elif data_char == ' ':\n",
    "            print(f'Space')\n",
    "            i_data += 1\n",
    "        elif data_char == '(':\n",
    "            print(f'Opening parenthesis. Matching tag')\n",
    "            i_data += 1\n",
    "            data_rem = data[i_data:]\n",
    "            i_clos_par = data_rem.find(')')\n",
    "            value = data_rem[:i_clos_par]\n",
    "            match = re.findall(pattern, data_rem[i_clos_par + 1 : i_clos_par + 6])[0]\n",
    "            matched_tag = [tag for tag in match if tag != ''][0]\n",
    "            slots[config.tag_to_name[matched_tag]] = value\n",
    "            print(f'Found tag {matched_tag!r} with value {value!r}')\n",
    "        elif data_char == ')':\n",
    "            print(f'Closing parenthesis. Resetting tag to {config.null_label!r}')\n",
    "            i_data += len(matched_tag)\n",
    "            i_data += 1\n",
    "            matched_tag = config.null_label\n",
    "        elif matched_tag != config.null_label:\n",
    "            print(f'Label token {token!r} as {matched_tag!r}')\n",
    "            labels[i_tokens] = matched_tag\n",
    "            i_data += len(token.strip('#'))\n",
    "            i_tokens += 1\n",
    "        else:\n",
    "            print(f'Skipping chars \\'', end='')\n",
    "            for token_char in token:\n",
    "                data_char = data[i_data]\n",
    "                if token_char == '#':\n",
    "                    print(f'#', end='')\n",
    "                elif token_char != data_char:\n",
    "                    raise ValueError(f'Token char {token_char!r} not equal to data char {data_char!r}')\n",
    "                else:\n",
    "                    print(f'{token_char}', end='')\n",
    "                    i_data += 1\n",
    "            print(f'\\'\\nToken end')\n",
    "            i_tokens += 1\n",
    "    labels_orig = deepcopy(labels)\n",
    "    for i_label in range(1, len(labels)):\n",
    "        if labels[i_label] == config.null_label:\n",
    "            pass\n",
    "        elif labels_orig[i_label - 1] != labels_orig[i_label]:\n",
    "            labels[i_label] = f'B-{labels[i_label]}'\n",
    "        elif labels_orig[i_label - 1] == labels_orig[i_label] != config.null_label:\n",
    "            labels[i_label] = f'I-{labels[i_label]}'\n",
    "    return text, tokens, labels, slots\n",
    "\n",
    "for query in queries_product_search:\n",
    "    print(f'{query=!r}')\n",
    "    data, product, price = re.findall(r'(.+?); (.+?); (\\d+)', query)[0]\n",
    "    price = int(price)\n",
    "    text, tokens, labels, slots = label_data(data)\n",
    "    slot_filling_dataset.append({\n",
    "        'raw_data': data,\n",
    "        'text': text,\n",
    "        'tokens': tokens,\n",
    "        'labels': labels,\n",
    "    })\n",
    "    retrieval_item = {\n",
    "        'raw_data': data,\n",
    "        'text': text,\n",
    "        'product': product,\n",
    "        'price': price,\n",
    "    }\n",
    "    for tag_name in ['category', 'brand', 'model']:\n",
    "        if tag_name in slots:\n",
    "            retrieval_item[tag_name] = slots[tag_name]\n",
    "    retrieval_dataset.append(retrieval_item)\n",
    "    intent_classification_dataset.append({\n",
    "        'text': text,\n",
    "        'label': 'product_search',\n",
    "    })\n",
    "for query in queries_product_info:\n",
    "    print(f'{query=}')\n",
    "    data, price = re.findall(r'(.+?); (\\d+)', query)[0]\n",
    "    price = int(price)\n",
    "    text, tokens, labels, slots = label_data(data)\n",
    "    slot_filling_dataset.append({\n",
    "        'raw_data': data,\n",
    "        'text': text,\n",
    "        'tokens': tokens,\n",
    "        'labels': labels,\n",
    "    })\n",
    "    retrieval_item = {\n",
    "        'raw_data': data,\n",
    "        'text': text,\n",
    "        'price': price,\n",
    "    }\n",
    "    for tag_name in ['brand', 'model']:\n",
    "        if tag_name in slots:\n",
    "            retrieval_item[tag_name] = slots[tag_name]\n",
    "    retrieval_dataset.append(retrieval_item)\n",
    "    intent_classification_dataset.append({\n",
    "        'text': text,\n",
    "        'label': 'product_info',\n",
    "    })"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a825d4fba3a57e4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('data/slot_filling_dataset.json', 'w') as file:\n",
    "    json.dump(slot_filling_dataset, file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f40644327aa76119"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('data/retrieval_dataset.json', 'w') as file:\n",
    "    json.dump(retrieval_dataset, file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b46276e55501696f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('data/intent_classification_dataset.json', 'w') as file:\n",
    "    json.dump(intent_classification_dataset, file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d983ccc5dd9695b7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = Dataset.from_list(slot_filling_dataset)\n",
    "dataset = dataset.remove_columns(['raw_data', 'tokens'])\n",
    "dataset = dataset.train_test_split(test_size=0.05)\n",
    "\n",
    "def preprocess_text(batch: list[str]):\n",
    "    return tokenizer(batch, **tokenizer_kwargs)\n",
    "\n",
    "dataset = dataset.map(lambda item: preprocess_text(item['text']), remove_columns=['text'], batched=True)\n",
    "\n",
    "def preprocess_labels(batch: list[list[str]]):\n",
    "    return {'labels': torch.tensor([[config.label_to_id[label] for label in labels] for labels in batch], \n",
    "                                   dtype=torch.int)}\n",
    "\n",
    "dataset = dataset.map(lambda item: preprocess_labels(item['labels']), remove_columns=['labels'], batched=True)\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2403461778d081a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset.set_format('torch')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2b559dd4e7e0c82"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset['train'], shuffle=True, batch_size=8)\n",
    "eval_dataloader = DataLoader(dataset['test'], batch_size=8)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "343c0f5f0c0fd067"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(checkpoint, num_labels=len(config.labels))\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f5abe00ebea9a1b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if not param.requires_grad:\n",
    "        print(name)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4bfbd5e09eeaa5d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23160d79712c883b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_weight = torch.full(size=(config.num_labels,), fill_value=1.0, dtype=torch.float).to(device)\n",
    "class_weight[config.label_to_id[config.null_label]] = 0.1\n",
    "\n",
    "cross_entropy = nn.CrossEntropyLoss(weight=class_weight)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28f7bc7cf4226207"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='linear', optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19e7ac024a767f26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        \n",
    "        logits = outputs['logits']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        loss = cross_entropy(logits.transpose(-1, -2), labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "562360cbe7f9b9cb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metric = evaluate.load('accuracy')\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    batch_predictions = torch.argmax(logits, dim=-1)\n",
    "    for token_ids, labels, predictions in zip(batch['input_ids'], batch['labels'], batch_predictions):\n",
    "        tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "        tokens = [token for token in tokens if token != '[PAD]']\n",
    "        labels = [config.id_to_label[label] for label in labels.tolist()]\n",
    "        predictions = [config.id_to_label[label] for label in predictions.tolist()]\n",
    "        result = pd.DataFrame(zip(tokens, labels, predictions), columns=['token', 'label', 'prediction'])\n",
    "        pprint(result)\n",
    "    \n",
    "    metric.add_batch(predictions=batch_predictions.view(-1), references=batch['labels'].view(-1))\n",
    "\n",
    "metric.compute()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2bd81817536bfaaa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save_pretrained('models/slot_filling')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6cca8c1c1c222b9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ad15c69c9fa67c42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer_kwargs = dict(return_tensors='pt', max_length=64, truncation=True, padding='max_length')\n",
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-multilingual-cased')\n",
    "\n",
    "def preprocess_text(batch: list[str]):\n",
    "    batch = tokenizer(batch, **tokenizer_kwargs)\n",
    "    return {k: v.to(device) for k, v in batch.items()}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3a27be07c9cf77b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained('models/slot_filling').to(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16ba273fb8004a5b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# query = 'Что включает Microsoft Surface Laptop 2?'\n",
    "query = 'Мне нужен смартфон Xiaomi до 15000, который имеет рейтинг 4.5.'\n",
    "\n",
    "tokens_ids = preprocess_text([query])\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokens_ids['input_ids'][0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3aa290edc3127ca1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**tokens_ids)\n",
    "logits = outputs.logits[0]\n",
    "predictions = torch.argmax(logits, dim=-1)\n",
    "predictions = [config.id_to_label[label] for label in predictions.tolist()]\n",
    "tokens_and_labels = list(zip(tokens, predictions))\n",
    "tokens_and_labels = [(token, label) for token, label in tokens_and_labels if token != '[PAD]']\n",
    "tags = {}\n",
    "i = 0\n",
    "while i < len(tokens_and_labels):\n",
    "    token, label = tokens_and_labels[i]\n",
    "    if label == config.null_label:\n",
    "        i += 1\n",
    "        continue\n",
    "    if label.startswith('B-'):\n",
    "        tag = label[2:]\n",
    "        if tag in tags:\n",
    "            continue\n",
    "        tags[tag] = [token]\n",
    "        i += 1\n",
    "        token, label = tokens_and_labels[i]\n",
    "        while label == f'I-{tag}':\n",
    "            tags[tag].append(token)\n",
    "            i += 1\n",
    "            token, label = tokens_and_labels[i]\n",
    "tags = {tag: tokenizer.decode(tokenizer.convert_tokens_to_ids(tokens)) for tag, tokens in tags.items()}\n",
    "tags"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4181a5cc53bf779"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ccb034ae2e4ad1d5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
